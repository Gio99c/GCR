{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Gio99c/GCR/blob/master/GCR_Lantent_conditional_sampling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clone the repository and install dependecies"
      ],
      "metadata": {
        "id": "TS_ofXaFTPPd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t8-KkjrFLJrZ"
      },
      "outputs": [],
      "source": [
        "!git clone -q https://github.com/Gio99c/GRC.git\n",
        "%cd -q GRC/main\n",
        "!pip install -q hydra\n",
        "!pip install -q torchdiffeq\n",
        "!pip install -q hydra-core\n",
        "!pip install -q pytorch-lightning\n",
        "!pip install -q wget"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GCR setup"
      ],
      "metadata": {
        "id": "_JQ4S-xITaSG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "uEHxdW0BmvnE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ec41084-d28c-4f52-923f-d5bb60372328"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:lightning_lite.utilities.seed:Global seed set to 0\n"
          ]
        }
      ],
      "source": [
        "%%capture\n",
        "import os\n",
        "import sys\n",
        "import torch\n",
        "import torchvision as tv\n",
        "p = os.path.join(os.path.abspath(\".\"), \"\")\n",
        "sys.path.insert(1, p)\n",
        "p = os.path.join(os.path.abspath(\".\"), \"lace\")\n",
        "sys.path.insert(2, p)\n",
        "\n",
        "from util import dotdict\n",
        "import wget\n",
        "from models.vae import VAE\n",
        "from models.diffusion import DDPM, DDPMv2, DDPMWrapper, SuperResModel, UNetModel\n",
        "from datasets import CIFAR10Dataset\n",
        "from datasets.latent import LatentDataset\n",
        "from lace.sampling import _sample_q_dict\n",
        "from lace.eval_sampler import ConditionalSampling\n",
        "import copy\n",
        "import hydra\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.utilities.seed import seed_everything\n",
        "from models.callbacks import ImageWriter\n",
        "from torch.utils.data import DataLoader\n",
        "from util import configure_device\n",
        "from models.diffusion.classifier import mobilenet_v2\n",
        "from models.diffusion.gcr import GCR\n",
        "from PIL import Image\n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "sqrt = lambda x: int(torch.sqrt(torch.Tensor([x])))\n",
        "plot = lambda p, x: tv.utils.save_image(torch.clamp(x, -1, 1), p, normalize=False, nrow=sqrt(x.size(0)))\n",
        "\n",
        "\n",
        "## Load pretrained classifier\n",
        "state_dict_clf = torch.load(\"./models/diffusion/state_dicts/mobilenet_v2.pt\")\n",
        "clf = mobilenet_v2(pretrained=True, progress=True, device=torch.device('cuda'))\n",
        "clf.load_state_dict(state_dict_clf)\n",
        "clf.eval()\n",
        "\n",
        "\n",
        "## Download pretrained VAE\n",
        "wget.download(\"https://drive.google.com/u/0/uc?id=1_UODuLoAhgAmxxMfNP5tG2ovJiRMGVS2&export=download&confirm=t&uuid=8e4b55c8-560a-4e59-86d1-de36667d5fe8&at=AHV7M3eUiPswRghe8ZrHKaWI8HK-:1670561868660\", \"./models/diffusion/state_dicts/vae_cifar10.ckpt\")\n",
        "vae = VAE.load_from_checkpoint(\"./models/diffusion/state_dicts/vae_cifar10.ckpt\", input_res=32)\n",
        "vae.eval()\n",
        "\n",
        "## Download pretrained DiffuseVAE\n",
        "wget.download(\"https://drive.google.com/u/0/uc?id=1QXVvJ0XyrPyD4hQr09B1gEVGzhmn2IuY&export=download&confirm=t&uuid=3869ccd1-8848-4e86-b997-ed082fc5ee3f&at=AHV7M3eK3aMh3gARBEJR50j0ECH4:1670562425982\", \"./models/diffusion/state_dicts/diffusevae_cifar10.ckpt\")\n",
        "config = dotdict(\n",
        "    {'dataset': dotdict(\n",
        "        {'ddpm': dotdict(\n",
        "            {'data': dotdict(\n",
        "                {'root': '???', 'name': 'cifar10', 'image_size': 32, 'hflip': True, 'n_channels': 3, 'norm': True, 'ddpm_latent_path': ''}), 'model': dotdict(\n",
        "                    {'dim': 160, 'attn_resolutions': '16,', 'n_residual': 3, 'dim_mults': '1,2,2,2', 'dropout': 0.3, 'n_heads': 8, 'beta1': 0.0001, 'beta2': 0.02, 'n_timesteps': 1000}), 'evaluation': dotdict(\n",
        "                        {'chkpt_path': './models/diffusion/state_dicts/diffusevae_cifar10.ckpt', 'save_path': '.', 'z_cond': False, 'z_dim': 512, 'guidance_weight': 0.0, 'type': 'form1', 'resample_strategy': 'truncated', 'skip_strategy': 'quad', 'sample_method': 'ddpm', 'sample_from': 'target', 'seed': 0, 'device': 'gpu', 'n_samples': 2500, 'n_steps': 1, 'workers': 4, 'batch_size': 2, 'save_vae': True, 'variance': 'fixedlarge', 'sample_prefix': 'gpu_1', 'temp': 1.0, 'save_mode': 'image'}), 'interpolation': dotdict(\n",
        "                            {'n_steps': 10})}), 'vae': dotdict({'data': dotdict({'root': '???', 'name': 'cifar10', 'image_size': 32, 'n_channels': 3}), 'model': dotdict({'z_dim': 512, 'enc_block_config': '32x7,32d2,32t16,16x4,16d2,16t8,8x4,8d2,8t4,4x3,4d4,4t1,1x3', 'enc_channel_config': '32:64,16:128,8:256,4:256,1:512', 'dec_block_config': '1x1,1u4,1t4,4x2,4u2,4t8,8x3,8u2,8t16,16x7,16u2,16t32,32x15', 'dec_channel_config': '32:64,16:128,8:256,4:256,1:512'}), 'evaluation': dotdict(\n",
        "                                {'chkpt_path': './models/diffusion/state_dicts/vae_cifar10.ckpt', 'save_path': '???', 'expde_model_path': '', 'seed': 0, 'device': 'gpu', 'workers': 2, 'batch_size': 8, 'n_samples': 50000, 'sample_prefix': '', 'save_mode': 'image'})})})})\n",
        "\n",
        "def __parse_str(s):\n",
        "    split = s.split(\",\")\n",
        "    return [int(s) for s in split if s != \"\" and s is not None]\n",
        "\n",
        "\n",
        "# Seed and setup\n",
        "config_ddpm = config.dataset.ddpm\n",
        "config_vae = config.dataset.vae\n",
        "seed_everything(config_ddpm.evaluation.seed, workers=True)\n",
        "\n",
        "batch_size = 256\n",
        "n_steps = config_ddpm.evaluation.n_steps\n",
        "n_samples = config_ddpm.evaluation.n_samples\n",
        "image_size = config_ddpm.data.image_size\n",
        "ddpm_latent_path = config_ddpm.data.ddpm_latent_path\n",
        "ddpm_latents = torch.load(ddpm_latent_path) if ddpm_latent_path != \"\" else None\n",
        "\n",
        "# Load pretrained VAE\n",
        "vae = VAE.load_from_checkpoint(\n",
        "    config_vae.evaluation.chkpt_path,\n",
        "    input_res=image_size,\n",
        ").cuda()\n",
        "vae.eval()\n",
        "\n",
        "# Load pretrained wrapper\n",
        "attn_resolutions = __parse_str(config_ddpm.model.attn_resolutions)\n",
        "dim_mults = __parse_str(config_ddpm.model.dim_mults)\n",
        "decoder = SuperResModel(\n",
        "    in_channels=config_ddpm.data.n_channels,\n",
        "    model_channels=config_ddpm.model.dim,\n",
        "    out_channels=3,\n",
        "    num_res_blocks=config_ddpm.model.n_residual,\n",
        "    attention_resolutions=attn_resolutions,\n",
        "    channel_mult=dim_mults,\n",
        "    use_checkpoint=False,\n",
        "    dropout=config_ddpm.model.dropout,\n",
        "    num_heads=config_ddpm.model.n_heads,\n",
        "    z_dim=config_ddpm.evaluation.z_dim,\n",
        "    use_scale_shift_norm=config_ddpm.evaluation.z_cond,\n",
        "    use_z=config_ddpm.evaluation.z_cond,\n",
        ")\n",
        "\n",
        "ema_decoder = copy.deepcopy(decoder)\n",
        "decoder.eval()\n",
        "ema_decoder.eval()\n",
        "\n",
        "ddpm_cls = DDPMv2 if config_ddpm.evaluation.type == \"form2\" else DDPM\n",
        "online_ddpm = ddpm_cls(\n",
        "    decoder,\n",
        "    beta_1=config_ddpm.model.beta1,\n",
        "    beta_2=config_ddpm.model.beta2,\n",
        "    T=config_ddpm.model.n_timesteps,\n",
        "    var_type=config_ddpm.evaluation.variance,\n",
        ")\n",
        "target_ddpm = ddpm_cls(\n",
        "    ema_decoder,\n",
        "    beta_1=config_ddpm.model.beta1,\n",
        "    beta_2=config_ddpm.model.beta2,\n",
        "    T=config_ddpm.model.n_timesteps,\n",
        "    var_type=config_ddpm.evaluation.variance,\n",
        ")\n",
        "\n",
        "ddpm_wrapper = DDPMWrapper.load_from_checkpoint(\n",
        "    config_ddpm.evaluation.chkpt_path,\n",
        "    online_network=online_ddpm,\n",
        "    target_network=target_ddpm,\n",
        "    vae=vae,\n",
        "    conditional=True,\n",
        "    pred_steps=n_steps,\n",
        "    eval_mode=\"sample\",\n",
        "    resample_strategy=config_ddpm.evaluation.resample_strategy,\n",
        "    skip_strategy=config_ddpm.evaluation.skip_strategy,\n",
        "    sample_method=config_ddpm.evaluation.sample_method,\n",
        "    sample_from=config_ddpm.evaluation.sample_from,\n",
        "    data_norm=config_ddpm.data.norm,\n",
        "    temp=config_ddpm.evaluation.temp,\n",
        "    guidance_weight=config_ddpm.evaluation.guidance_weight,\n",
        "    z_cond=config_ddpm.evaluation.z_cond,\n",
        "    strict=True,\n",
        "    ddpm_latents=ddpm_latents,\n",
        ")\n",
        "\n",
        "\n",
        "model = GCR(ddpm_wrapper.to(device), clf.to(device), config_vae.model.z_dim)\n",
        "\n",
        "sample_q = _sample_q_dict[\"sample_q_ode\"]\n",
        "ode_kwargs = {'atol': 1e-4, 'rtol': 1e-4, 'method': \"dopri5\", 'use_adjoint': True}\n",
        "ld_kwargs = {'batch_size': batch_size, 'latent_dim': config_vae.model.z_dim, 'sgld_lr': 1,\n",
        "                'sgld_std': 1e-2, 'n_steps': 1}\n",
        "sde_kwargs = {'N': 100, 'correct_nsteps': 2, 'target_snr': 0.16}\n",
        "\n",
        "n_classes = 10\n",
        "\n",
        "condSampling = ConditionalSampling(sample_q, 256, config_vae.model.z_dim, n_classes, model,\n",
        "                                    device, \".\", ode_kwargs, ld_kwargs, sde_kwargs,\n",
        "                                    every_n_plot=1)\n",
        "\n",
        "# Create predict dataset of latents\n",
        "z_dataset = LatentDataset(\n",
        "    (n_samples, config_vae.model.z_dim, 1, 1),\n",
        "    (n_samples, 3, image_size, image_size),\n",
        "    share_ddpm_latent=True if ddpm_latent_path != \"\" else False,\n",
        "    expde_model_path=config_vae.evaluation.expde_model_path,\n",
        "    seed=config_ddpm.evaluation.seed,\n",
        ")\n",
        "\n",
        "# Setup devices\n",
        "test_kwargs = {}\n",
        "loader_kws = {}\n",
        "device = 'cuda'\n",
        "if torch.cuda.is_available():\n",
        "    test_kwargs['accelerator'] = \"gpu\"\n",
        "    test_kwargs[\"gpus\"] = 1\n",
        "    \n",
        "\n",
        "    # Disable find_unused_parameters when using DDP training for performance reasons\n",
        "    loader_kws[\"persistent_workers\"] = True\n",
        "\n",
        "\n",
        "# Predict loader\n",
        "#pin_memory commented\n",
        "val_loader = DataLoader(\n",
        "    z_dataset,\n",
        "    batch_size=batch_size,\n",
        "    drop_last=False,\n",
        "    shuffle=False,\n",
        "    pin_memory=True,\n",
        "    num_workers=config_ddpm.evaluation.workers,\n",
        "    **loader_kws,\n",
        ")\n",
        "\n",
        "# Predict trainer\n",
        "write_callback = ImageWriter(\n",
        "    config_ddpm.evaluation.save_path,\n",
        "    \"batch\",\n",
        "    n_steps=n_steps,\n",
        "    eval_mode=\"sample\",\n",
        "    conditional=True,\n",
        "    sample_prefix=config_ddpm.evaluation.sample_prefix,\n",
        "    save_vae=config_ddpm.evaluation.save_vae,\n",
        "    save_mode=config_ddpm.evaluation.save_mode,\n",
        "    is_norm=config_ddpm.data.norm,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conditional generation"
      ],
      "metadata": {
        "id": "JNknnjW_ThIU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classes = {\"airplane\": 0, \"automobile\": 1, \"bird\": 2, \"cat\": 3, \"deer\": 4, \"dog\": 5, \"frog\": 6, \"horse\": 7, \"ship\": 8, \"truck\": 9}\n",
        "sampling = {\"single\": False, \"double\": True}\n",
        "\n",
        "j = classes[input(\"Select the class for conditional generation (airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck): \")]\n",
        "double = sampling[input(\"Do you want to perform single or double sampling? (just type 'single' or 'double'): \")]\n",
        "for i, (x_t, z) in enumerate(val_loader):\n",
        "  x_t = x_t.to(device)\n",
        "  z = z.to(device)\n",
        "  torch.set_grad_enabled(False)\n",
        "  model.g.init_lace(c=torch.ones((1), dtype=torch.int64, device=device) * j, sampler=condSampling, double=double)\n",
        "  img = model.g(x=x_t, z=z, n_steps=1000)['1000']\n",
        "  plot(f\"./batch{i}.png\", img)\n",
        "  print(f\"Batch {i}\")\n",
        "  Image.open(f\"./batch{i}.png\")"
      ],
      "metadata": {
        "id": "J2HUgARMTwOf"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyP6+a7Q9YHDTY8CzoizkwzN",
      "include_colab_link": true
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}