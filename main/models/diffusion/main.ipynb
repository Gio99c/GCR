{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "p = os.path.join(os.path.abspath(\"../..\"), \"main\")\n",
    "sys.path.insert(1, p)\n",
    "\n",
    "from models.diffusion.classifier import mobilenet_v2\n",
    "from models.diffusion.gcr import GCR\n",
    "import hydra\n",
    "from util import dotdict\n",
    "from models.vae import VAE\n",
    "from models.diffusion import DDPM, DDPMv2, DDPMWrapper, SuperResModel, UNetModel\n",
    "from datasets import CIFAR10Dataset\n",
    "from LACE.sampling import _sample_q_dict\n",
    "from LACE.eval_sampler import ConditionalSampling\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "state_dict_clf = torch.load(\"/Users/gio/Desktop/UIC/CS594/Class project/GRC/main/models/diffusion/state_dicts/mobilenet_v2.pt\")\n",
    "clf = mobilenet_v2(pretrained=True, progress=True, device=\"cpu\")\n",
    "clf.load_state_dict(state_dict_clf)\n",
    "clf.eval()\n",
    "\n",
    "\n",
    "\n",
    "vae = VAE.load_from_checkpoint(\n",
    "        \"/Users/gio/Desktop/UIC/CS594/Class project/GRC/main/checkpoints/vae_cifar10_loss=0.00.ckpt\",\n",
    "        input_res=32,\n",
    "    )\n",
    "vae.eval()\n",
    "\n",
    "\n",
    "config = dotdict({'dataset': dotdict({'ddpm': dotdict({'data': dotdict({'root': '???', 'name': 'cifar10', 'image_size': 32, 'hflip': True, 'n_channels': 3, 'norm': True, 'ddpm_latent_path': ''}), 'model': dotdict({'dim': 160, 'attn_resolutions': '16,', 'n_residual': 3, 'dim_mults': '1,2,2,2', 'dropout': 0.3, 'n_heads': 8, 'beta1': 0.0001, 'beta2': 0.02, 'n_timesteps': 1000}), 'evaluation': dotdict({'chkpt_path': '/Users/gio/Desktop/UIC/CS594/Class project/GRC/main/checkpoints/diffvae_cifar10_form1_largermodel_loss=0.0438.ckpt', 'save_path': '.', 'z_cond': False, 'z_dim': 512, 'guidance_weight': 0.0, 'type': 'form1', 'resample_strategy': 'truncated', 'skip_strategy': 'quad', 'sample_method': 'ddpm', 'sample_from': 'target', 'seed': 0, 'device': 'cpu', 'n_samples': 2500, 'n_steps': 1000, 'workers': 4, 'batch_size': 2, 'save_vae': True, 'variance': 'fixedlarge', 'sample_prefix': 'gpu_1', 'temp': 1.0, 'save_mode': 'image'}), 'interpolation': dotdict({'n_steps': 10})}), 'vae': dotdict({'data': dotdict({'root': '???', 'name': 'cifar10', 'image_size': 32, 'n_channels': 3}), 'model': dotdict({'z_dim': 512, 'enc_block_config': '32x7,32d2,32t16,16x4,16d2,16t8,8x4,8d2,8t4,4x3,4d4,4t1,1x3', 'enc_channel_config': '32:64,16:128,8:256,4:256,1:512', 'dec_block_config': '1x1,1u4,1t4,4x2,4u2,4t8,8x3,8u2,8t16,16x7,16u2,16t32,32x15', 'dec_channel_config': '32:64,16:128,8:256,4:256,1:512'}), 'evaluation': dotdict({'chkpt_path': '/Users/gio/Desktop/UIC/CS594/Class project/GRC/main/checkpoints/vae_cifar10_loss=0.00.ckpt', 'save_path': '???', 'expde_model_path': '', 'seed': 0, 'device': 'gpu:0', 'workers': 2, 'batch_size': 8, 'n_samples': 50000, 'sample_prefix': '', 'save_mode': 'image'})})})})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gio/opt/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/seed.py:48: LightningDeprecationWarning: `pytorch_lightning.utilities.seed.seed_everything` has been deprecated in v1.8.0 and will be removed in v1.10.0. Please use `lightning_lite.utilities.seed.seed_everything` instead.\n",
      "  rank_zero_deprecation(\n",
      "Global seed set to 0\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "import hydra\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from datasets.latent import LatentDataset\n",
    "from models.callbacks import ImageWriter\n",
    "from models.diffusion import DDPM, DDPMv2, DDPMWrapper, SuperResModel\n",
    "from models.vae import VAE\n",
    "from pytorch_lightning.utilities.seed import seed_everything\n",
    "from torch.utils.data import DataLoader\n",
    "from util import configure_device\n",
    "\n",
    "\n",
    "def __parse_str(s):\n",
    "    split = s.split(\",\")\n",
    "    return [int(s) for s in split if s != \"\" and s is not None]\n",
    "\n",
    "\n",
    "\n",
    "# Seed and setup\n",
    "config_ddpm = config.dataset.ddpm\n",
    "config_vae = config.dataset.vae\n",
    "seed_everything(config_ddpm.evaluation.seed, workers=True)\n",
    "\n",
    "batch_size = 256#config_ddpm.evaluation.batch_size\n",
    "n_steps = config_ddpm.evaluation.n_steps\n",
    "n_samples = config_ddpm.evaluation.n_samples\n",
    "image_size = config_ddpm.data.image_size\n",
    "ddpm_latent_path = config_ddpm.data.ddpm_latent_path\n",
    "ddpm_latents = torch.load(ddpm_latent_path) if ddpm_latent_path != \"\" else None\n",
    "\n",
    "# Load pretrained VAE\n",
    "vae = VAE.load_from_checkpoint(\n",
    "    config_vae.evaluation.chkpt_path,\n",
    "    input_res=image_size,\n",
    ")\n",
    "vae.eval()\n",
    "\n",
    "# Load pretrained wrapper\n",
    "attn_resolutions = __parse_str(config_ddpm.model.attn_resolutions)\n",
    "dim_mults = __parse_str(config_ddpm.model.dim_mults)\n",
    "decoder = SuperResModel(\n",
    "    in_channels=config_ddpm.data.n_channels,\n",
    "    model_channels=config_ddpm.model.dim,\n",
    "    out_channels=3,\n",
    "    num_res_blocks=config_ddpm.model.n_residual,\n",
    "    attention_resolutions=attn_resolutions,\n",
    "    channel_mult=dim_mults,\n",
    "    use_checkpoint=False,\n",
    "    dropout=config_ddpm.model.dropout,\n",
    "    num_heads=config_ddpm.model.n_heads,\n",
    "    z_dim=config_ddpm.evaluation.z_dim,\n",
    "    use_scale_shift_norm=config_ddpm.evaluation.z_cond,\n",
    "    use_z=config_ddpm.evaluation.z_cond,\n",
    ")\n",
    "\n",
    "ema_decoder = copy.deepcopy(decoder)\n",
    "decoder.eval()\n",
    "ema_decoder.eval()\n",
    "\n",
    "ddpm_cls = DDPMv2 if config_ddpm.evaluation.type == \"form2\" else DDPM\n",
    "online_ddpm = ddpm_cls(\n",
    "    decoder,\n",
    "    beta_1=config_ddpm.model.beta1,\n",
    "    beta_2=config_ddpm.model.beta2,\n",
    "    T=config_ddpm.model.n_timesteps,\n",
    "    var_type=config_ddpm.evaluation.variance,\n",
    ")\n",
    "target_ddpm = ddpm_cls(\n",
    "    ema_decoder,\n",
    "    beta_1=config_ddpm.model.beta1,\n",
    "    beta_2=config_ddpm.model.beta2,\n",
    "    T=config_ddpm.model.n_timesteps,\n",
    "    var_type=config_ddpm.evaluation.variance,\n",
    ")\n",
    "\n",
    "ddpm_wrapper = DDPMWrapper.load_from_checkpoint(\n",
    "    config_ddpm.evaluation.chkpt_path,\n",
    "    online_network=online_ddpm,\n",
    "    target_network=target_ddpm,\n",
    "    vae=vae,\n",
    "    conditional=True,\n",
    "    pred_steps=n_steps,\n",
    "    eval_mode=\"sample\",\n",
    "    resample_strategy=config_ddpm.evaluation.resample_strategy,\n",
    "    skip_strategy=config_ddpm.evaluation.skip_strategy,\n",
    "    sample_method=config_ddpm.evaluation.sample_method,\n",
    "    sample_from=config_ddpm.evaluation.sample_from,\n",
    "    data_norm=config_ddpm.data.norm,\n",
    "    temp=config_ddpm.evaluation.temp,\n",
    "    guidance_weight=config_ddpm.evaluation.guidance_weight,\n",
    "    z_cond=config_ddpm.evaluation.z_cond,\n",
    "    strict=True,\n",
    "    ddpm_latents=ddpm_latents,\n",
    ")\n",
    "\n",
    "# Create predict dataset of latents\n",
    "z_dataset = LatentDataset(\n",
    "    (n_samples, config_vae.model.z_dim, 1, 1),\n",
    "    (n_samples, 3, image_size, image_size),\n",
    "    share_ddpm_latent=True if ddpm_latent_path != \"\" else False,\n",
    "    expde_model_path=config_vae.evaluation.expde_model_path,\n",
    "    seed=config_ddpm.evaluation.seed,\n",
    ")\n",
    "\n",
    "# Setup devices\n",
    "test_kwargs = {}\n",
    "loader_kws = {}\n",
    "device = config_ddpm.evaluation.device\n",
    "if device.startswith(\"gpu\"):\n",
    "    _, devs = configure_device(device)\n",
    "    test_kwargs['accelerator'] = \"gpu\"\n",
    "    test_kwargs[\"gpus\"] = devs\n",
    "\n",
    "    # Disable find_unused_parameters when using DDP training for performance reasons\n",
    "    loader_kws[\"persistent_workers\"] = True\n",
    "elif device == \"tpu\":\n",
    "    test_kwargs[\"tpu_cores\"] = 8\n",
    "\n",
    "# Predict loader\n",
    "#pin_memory commented\n",
    "val_loader = DataLoader(\n",
    "    z_dataset,\n",
    "    batch_size=batch_size,\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=config_ddpm.evaluation.workers,\n",
    "    **loader_kws,\n",
    ")\n",
    "\n",
    "# Predict trainer\n",
    "write_callback = ImageWriter(\n",
    "    config_ddpm.evaluation.save_path,\n",
    "    \"batch\",\n",
    "    n_steps=n_steps,\n",
    "    eval_mode=\"sample\",\n",
    "    conditional=True,\n",
    "    sample_prefix=config_ddpm.evaluation.sample_prefix,\n",
    "    save_vae=config_ddpm.evaluation.save_vae,\n",
    "    save_mode=config_ddpm.evaluation.save_mode,\n",
    "    is_norm=config_ddpm.data.norm,\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GCR(ddpm_wrapper, clf, config_vae.model.z_dim)\n",
    "\n",
    "sample_q = _sample_q_dict[\"sample_q_sgld\"]\n",
    "c = torch.ones((1), dtype=torch.int64)\n",
    "ode_kwargs = {'atol': 1e-3, 'rtol': 1e-3, 'method': \"dopri5\", 'use_adjoint': True}\n",
    "ld_kwargs = {'batch_size': batch_size, 'latent_dim': config_vae.model.z_dim, 'sgld_lr': 1,\n",
    "                'sgld_std': 1e-2, 'n_steps': 20}\n",
    "sde_kwargs = {'N': 1000, 'correct_nsteps': 2, 'target_snr': 0.16}\n",
    "\n",
    "n_classes = 10\n",
    "condSampling = ConditionalSampling(sample_q, batch_size, config_vae.model.z_dim, n_classes, model,\n",
    "                                    device, \".\", ode_kwargs, ld_kwargs, sde_kwargs,\n",
    "                                    every_n_plot=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [256, 512]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m condSampling\u001b[39m.\u001b[39;49mget_samples()\n",
      "File \u001b[0;32m~/Desktop/UIC/CS594/Class project/GRC/main/LACE/eval_sampler.py:112\u001b[0m, in \u001b[0;36mConditionalSampling.get_samples\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_classes):\n\u001b[1;32m    111\u001b[0m     y \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([i])\u001b[39m.\u001b[39mrepeat(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_size)\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m--> 112\u001b[0m     img, sample_time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sample_batch(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msampler, y, save_path\u001b[39m=\u001b[39;49msave_path)\n\u001b[1;32m    113\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mclass \u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m, sampling time: \u001b[39m\u001b[39m{\u001b[39;00msample_time\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m    114\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mplot(\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m/samples_class\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.png\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(save_path, i), img)\n",
      "File \u001b[0;32m~/Desktop/UIC/CS594/Class project/GRC/main/LACE/eval_sampler.py:48\u001b[0m, in \u001b[0;36mSampling._sample_batch\u001b[0;34m(self, sampler, y, save_path)\u001b[0m\n\u001b[1;32m     46\u001b[0m start_sample_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m     47\u001b[0m z_sampled \u001b[39m=\u001b[39m sampler(y\u001b[39m=\u001b[39my, save_path\u001b[39m=\u001b[39msave_path)\n\u001b[0;32m---> 48\u001b[0m g_z_sampled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mccf\u001b[39m.\u001b[39;49mg\u001b[39m.\u001b[39;49mvae(z_sampled)\n\u001b[1;32m     49\u001b[0m img \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mccf\u001b[39m.\u001b[39mgenerate_images(g_z_sampled)\n\u001b[1;32m     50\u001b[0m sample_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_sample_time\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Desktop/UIC/CS594/Class project/GRC/main/models/vae.py:238\u001b[0m, in \u001b[0;36mVAE.forward\u001b[0;34m(self, z)\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, z):\n\u001b[1;32m    237\u001b[0m     \u001b[39m# Only sample during inference\u001b[39;00m\n\u001b[0;32m--> 238\u001b[0m     decoder_out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecode(z)\n\u001b[1;32m    239\u001b[0m     \u001b[39mreturn\u001b[39;00m decoder_out\n",
      "File \u001b[0;32m~/Desktop/UIC/CS594/Class project/GRC/main/models/vae.py:226\u001b[0m, in \u001b[0;36mVAE.decode\u001b[0;34m(self, z)\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecode\u001b[39m(\u001b[39mself\u001b[39m, z):\n\u001b[0;32m--> 226\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdec(z)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Desktop/UIC/CS594/Class project/GRC/main/models/vae.py:182\u001b[0m, in \u001b[0;36mDecoder.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m--> 182\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mblock_mod(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    183\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlast_conv(x)\n\u001b[1;32m    184\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39msigmoid(x)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/container.py:141\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    140\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 141\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    142\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Desktop/UIC/CS594/Class project/GRC/main/models/vae.py:95\u001b[0m, in \u001b[0;36mResBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> 95\u001b[0m     xhat \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mc1(F\u001b[39m.\u001b[39;49mgelu(x))\n\u001b[1;32m     96\u001b[0m     xhat \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mc2(F\u001b[39m.\u001b[39mgelu(xhat))\n\u001b[1;32m     97\u001b[0m     xhat \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mc3(F\u001b[39m.\u001b[39mgelu(xhat))\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/conv.py:447\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 447\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/conv.py:443\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    440\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    441\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    442\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 443\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    444\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [256, 512]"
     ]
    }
   ],
   "source": [
    "condSampling.get_samples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for z_ddpm, z_vae in val_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2, 16384])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.generate_images(torch.rand((512,2,512))).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6ebd908b7e74dac9a348e9d3abf074a61c1e7e8efc0ae4932d05c55de7bf9a20"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
