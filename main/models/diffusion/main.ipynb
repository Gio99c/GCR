{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "p = os.path.join(os.path.abspath(\"../..\"), \"main\")\n",
    "sys.path.insert(1, p)\n",
    "\n",
    "from models.diffusion.classifier import mobilenet_v2\n",
    "from models.diffusion.gcr import GCR\n",
    "import hydra\n",
    "from util import dotdict\n",
    "from models.vae import VAE\n",
    "from models.diffusion import DDPM, DDPMv2, DDPMWrapper, SuperResModel, UNetModel\n",
    "from datasets import CIFAR10Dataset\n",
    "from LACE.sampling import _sample_q_dict\n",
    "from LACE.eval_sampler import ConditionalSampling\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "state_dict_clf = torch.load(\"/Users/gio/Desktop/UIC/CS594/Class project/GRC/main/models/diffusion/state_dicts/mobilenet_v2.pt\")\n",
    "clf = mobilenet_v2(pretrained=True, progress=True, device=\"cpu\")\n",
    "clf.load_state_dict(state_dict_clf)\n",
    "clf.eval()\n",
    "\n",
    "\n",
    "\n",
    "vae = VAE.load_from_checkpoint(\n",
    "        \"/Users/gio/Desktop/UIC/CS594/Class project/GRC/main/checkpoints/vae_cifar10_loss=0.00.ckpt\",\n",
    "        input_res=32,\n",
    "    )\n",
    "vae.eval()\n",
    "\n",
    "\n",
    "config = dotdict({'dataset': dotdict({'ddpm': dotdict({'data': dotdict({'root': '???', 'name': 'cifar10', 'image_size': 32, 'hflip': True, 'n_channels': 3, 'norm': True, 'ddpm_latent_path': ''}), 'model': dotdict({'dim': 160, 'attn_resolutions': '16,', 'n_residual': 3, 'dim_mults': '1,2,2,2', 'dropout': 0.3, 'n_heads': 8, 'beta1': 0.0001, 'beta2': 0.02, 'n_timesteps': 1000}), 'evaluation': dotdict({'chkpt_path': '/Users/gio/Desktop/UIC/CS594/Class project/GRC/main/checkpoints/diffvae_cifar10_form1_largermodel_loss=0.0438.ckpt', 'save_path': '.', 'z_cond': False, 'z_dim': 512, 'guidance_weight': 0.0, 'type': 'form1', 'resample_strategy': 'truncated', 'skip_strategy': 'quad', 'sample_method': 'ddpm', 'sample_from': 'target', 'seed': 0, 'device': 'cpu', 'n_samples': 2500, 'n_steps': 1000, 'workers': 4, 'batch_size': 2, 'save_vae': True, 'variance': 'fixedlarge', 'sample_prefix': 'gpu_1', 'temp': 1.0, 'save_mode': 'image'}), 'interpolation': dotdict({'n_steps': 10})}), 'vae': dotdict({'data': dotdict({'root': '???', 'name': 'cifar10', 'image_size': 32, 'n_channels': 3}), 'model': dotdict({'z_dim': 512, 'enc_block_config': '32x7,32d2,32t16,16x4,16d2,16t8,8x4,8d2,8t4,4x3,4d4,4t1,1x3', 'enc_channel_config': '32:64,16:128,8:256,4:256,1:512', 'dec_block_config': '1x1,1u4,1t4,4x2,4u2,4t8,8x3,8u2,8t16,16x7,16u2,16t32,32x15', 'dec_channel_config': '32:64,16:128,8:256,4:256,1:512'}), 'evaluation': dotdict({'chkpt_path': '/Users/gio/Desktop/UIC/CS594/Class project/GRC/main/checkpoints/vae_cifar10_loss=0.00.ckpt', 'save_path': '???', 'expde_model_path': '', 'seed': 0, 'device': 'gpu:0', 'workers': 2, 'batch_size': 8, 'n_samples': 50000, 'sample_prefix': '', 'save_mode': 'image'})})})})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gio/opt/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/seed.py:48: LightningDeprecationWarning: `pytorch_lightning.utilities.seed.seed_everything` has been deprecated in v1.8.0 and will be removed in v1.10.0. Please use `lightning_lite.utilities.seed.seed_everything` instead.\n",
      "  rank_zero_deprecation(\n",
      "Global seed set to 0\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "import hydra\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from datasets.latent import LatentDataset\n",
    "from models.callbacks import ImageWriter\n",
    "from models.diffusion import DDPM, DDPMv2, DDPMWrapper, SuperResModel\n",
    "from models.vae import VAE\n",
    "from pytorch_lightning.utilities.seed import seed_everything\n",
    "from torch.utils.data import DataLoader\n",
    "from util import configure_device\n",
    "\n",
    "\n",
    "def __parse_str(s):\n",
    "    split = s.split(\",\")\n",
    "    return [int(s) for s in split if s != \"\" and s is not None]\n",
    "\n",
    "\n",
    "\n",
    "# Seed and setup\n",
    "config_ddpm = config.dataset.ddpm\n",
    "config_vae = config.dataset.vae\n",
    "seed_everything(config_ddpm.evaluation.seed, workers=True)\n",
    "\n",
    "batch_size = 256#config_ddpm.evaluation.batch_size\n",
    "n_steps = config_ddpm.evaluation.n_steps\n",
    "n_samples = config_ddpm.evaluation.n_samples\n",
    "image_size = config_ddpm.data.image_size\n",
    "ddpm_latent_path = config_ddpm.data.ddpm_latent_path\n",
    "ddpm_latents = torch.load(ddpm_latent_path) if ddpm_latent_path != \"\" else None\n",
    "\n",
    "# Load pretrained VAE\n",
    "vae = VAE.load_from_checkpoint(\n",
    "    config_vae.evaluation.chkpt_path,\n",
    "    input_res=image_size,\n",
    ")\n",
    "vae.eval()\n",
    "\n",
    "# Load pretrained wrapper\n",
    "attn_resolutions = __parse_str(config_ddpm.model.attn_resolutions)\n",
    "dim_mults = __parse_str(config_ddpm.model.dim_mults)\n",
    "decoder = SuperResModel(\n",
    "    in_channels=config_ddpm.data.n_channels,\n",
    "    model_channels=config_ddpm.model.dim,\n",
    "    out_channels=3,\n",
    "    num_res_blocks=config_ddpm.model.n_residual,\n",
    "    attention_resolutions=attn_resolutions,\n",
    "    channel_mult=dim_mults,\n",
    "    use_checkpoint=False,\n",
    "    dropout=config_ddpm.model.dropout,\n",
    "    num_heads=config_ddpm.model.n_heads,\n",
    "    z_dim=config_ddpm.evaluation.z_dim,\n",
    "    use_scale_shift_norm=config_ddpm.evaluation.z_cond,\n",
    "    use_z=config_ddpm.evaluation.z_cond,\n",
    ")\n",
    "\n",
    "ema_decoder = copy.deepcopy(decoder)\n",
    "decoder.eval()\n",
    "ema_decoder.eval()\n",
    "\n",
    "ddpm_cls = DDPMv2 if config_ddpm.evaluation.type == \"form2\" else DDPM\n",
    "online_ddpm = ddpm_cls(\n",
    "    decoder,\n",
    "    beta_1=config_ddpm.model.beta1,\n",
    "    beta_2=config_ddpm.model.beta2,\n",
    "    T=config_ddpm.model.n_timesteps,\n",
    "    var_type=config_ddpm.evaluation.variance,\n",
    ")\n",
    "target_ddpm = ddpm_cls(\n",
    "    ema_decoder,\n",
    "    beta_1=config_ddpm.model.beta1,\n",
    "    beta_2=config_ddpm.model.beta2,\n",
    "    T=config_ddpm.model.n_timesteps,\n",
    "    var_type=config_ddpm.evaluation.variance,\n",
    ")\n",
    "\n",
    "ddpm_wrapper = DDPMWrapper.load_from_checkpoint(\n",
    "    config_ddpm.evaluation.chkpt_path,\n",
    "    online_network=online_ddpm,\n",
    "    target_network=target_ddpm,\n",
    "    vae=vae,\n",
    "    conditional=True,\n",
    "    pred_steps=n_steps,\n",
    "    eval_mode=\"sample\",\n",
    "    resample_strategy=config_ddpm.evaluation.resample_strategy,\n",
    "    skip_strategy=config_ddpm.evaluation.skip_strategy,\n",
    "    sample_method=config_ddpm.evaluation.sample_method,\n",
    "    sample_from=config_ddpm.evaluation.sample_from,\n",
    "    data_norm=config_ddpm.data.norm,\n",
    "    temp=config_ddpm.evaluation.temp,\n",
    "    guidance_weight=config_ddpm.evaluation.guidance_weight,\n",
    "    z_cond=config_ddpm.evaluation.z_cond,\n",
    "    strict=True,\n",
    "    ddpm_latents=ddpm_latents,\n",
    ")\n",
    "\n",
    "# Create predict dataset of latents\n",
    "z_dataset = LatentDataset(\n",
    "    (n_samples, config_vae.model.z_dim, 1, 1),\n",
    "    (n_samples, 3, image_size, image_size),\n",
    "    share_ddpm_latent=True if ddpm_latent_path != \"\" else False,\n",
    "    expde_model_path=config_vae.evaluation.expde_model_path,\n",
    "    seed=config_ddpm.evaluation.seed,\n",
    ")\n",
    "\n",
    "# Setup devices\n",
    "test_kwargs = {}\n",
    "loader_kws = {}\n",
    "device = config_ddpm.evaluation.device\n",
    "if device.startswith(\"gpu\"):\n",
    "    _, devs = configure_device(device)\n",
    "    test_kwargs['accelerator'] = \"gpu\"\n",
    "    test_kwargs[\"gpus\"] = devs\n",
    "\n",
    "    # Disable find_unused_parameters when using DDP training for performance reasons\n",
    "    loader_kws[\"persistent_workers\"] = True\n",
    "elif device == \"tpu\":\n",
    "    test_kwargs[\"tpu_cores\"] = 8\n",
    "\n",
    "# Predict loader\n",
    "#pin_memory commented\n",
    "val_loader = DataLoader(\n",
    "    z_dataset,\n",
    "    batch_size=batch_size,\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=config_ddpm.evaluation.workers,\n",
    "    **loader_kws,\n",
    ")\n",
    "\n",
    "# Predict trainer\n",
    "write_callback = ImageWriter(\n",
    "    config_ddpm.evaluation.save_path,\n",
    "    \"batch\",\n",
    "    n_steps=n_steps,\n",
    "    eval_mode=\"sample\",\n",
    "    conditional=True,\n",
    "    sample_prefix=config_ddpm.evaluation.sample_prefix,\n",
    "    save_vae=config_ddpm.evaluation.save_vae,\n",
    "    save_mode=config_ddpm.evaluation.save_mode,\n",
    "    is_norm=config_ddpm.data.norm,\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GCR(ddpm_wrapper, clf, config_vae.model.z_dim)\n",
    "\n",
    "sample_q = _sample_q_dict[\"sample_q_sgld\"]\n",
    "c = torch.ones((1), dtype=torch.int64)\n",
    "ode_kwargs = {'atol': 1e-3, 'rtol': 1e-3, 'method': \"dopri5\", 'use_adjoint': True}\n",
    "ld_kwargs = {'batch_size': batch_size, 'latent_dim': config_vae.model.z_dim, 'sgld_lr': 1,\n",
    "                'sgld_std': 1e-2, 'n_steps': 20}\n",
    "sde_kwargs = {'N': 1000, 'correct_nsteps': 2, 'target_snr': 0.16}\n",
    "\n",
    "n_classes = 10\n",
    "condSampling = ConditionalSampling(sample_q, batch_size, config_vae.model.z_dim, n_classes, model,\n",
    "                                    device, \".\", ode_kwargs, ld_kwargs, sde_kwargs,\n",
    "                                    every_n_plot=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "condSampling.get_samples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'latent_dim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [23], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfor\u001b[39;00m z_ddpm, z_vae \u001b[39min\u001b[39;00m val_loader:\n\u001b[0;32m----> 2\u001b[0m     sample_q(model, c, device\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mdevice(\u001b[39m'\u001b[39;49m\u001b[39mcpu\u001b[39;49m\u001b[39m'\u001b[39;49m), save_path\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m\"\u001b[39;49m, plot\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m, every_n_plot\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, kwargs\u001b[39m=\u001b[39;49m{\u001b[39m\"\u001b[39;49m\u001b[39mlatent_dim\u001b[39;49m\u001b[39m\"\u001b[39;49m : config_vae\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mz_dim})\n\u001b[1;32m      3\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/UIC/CS594/Class project/GRC/main/LACE/sampling.py:141\u001b[0m, in \u001b[0;36msample_q_sgld\u001b[0;34m(ccf, y, device, save_path, plot, every_n_plot, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[39m\"\"\"sampling in the z space\"\"\"\u001b[39;00m\n\u001b[1;32m    139\u001b[0m ccf\u001b[39m.\u001b[39meval()\n\u001b[0;32m--> 141\u001b[0m latent_dim \u001b[39m=\u001b[39m kwargs[\u001b[39m'\u001b[39;49m\u001b[39mlatent_dim\u001b[39;49m\u001b[39m'\u001b[39;49m]\n\u001b[1;32m    142\u001b[0m sgld_lr \u001b[39m=\u001b[39m kwargs[\u001b[39m'\u001b[39m\u001b[39msgld_lr\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m    143\u001b[0m sgld_std \u001b[39m=\u001b[39m kwargs[\u001b[39m'\u001b[39m\u001b[39msgld_std\u001b[39m\u001b[39m'\u001b[39m]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'latent_dim'"
     ]
    }
   ],
   "source": [
    "for z_ddpm, z_vae in val_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2, 16384])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.generate_images(torch.rand((512,2,512))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6ebd908b7e74dac9a348e9d3abf074a61c1e7e8efc0ae4932d05c55de7bf9a20"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
